{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e452ed9b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 5: Uncertainty</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aea4b2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e2407",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_1\">L5.1 What Do We Call Uncertainty?</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_1\">L5.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_2\">L5.2 Extracting Uncertainty For Linear Fit</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_2\">L5.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_3\">L5.3 Computing Uncertainty</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_3\">L5.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_4\">L5.4 Introduction to Likelihood</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_4\">L5.4 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_5\">L5.5 An Example: Auger Data (Part 1)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_5\">L5.5 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_6\">L5.6 An Example: Auger Data (Part 2)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_6\">L5.6 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_7\">L5.7 Log-Likelihood and Chi-square</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_7\">L5.7 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_8\">L5.8 Minimizing</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_8\">L5.8 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_9\">L5.9 Comparison Using lmfit</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_9\">L5.9 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a42e3",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "In this Lesson, we are going to understand what uncertainty is, and how uncertainty is obtained. For this Lesson, we will focus on data from the Auger Experiment, which measures high energy cosmic rays.\n",
    "\n",
    "We will explore the following topics:\n",
    "\n",
    "- What are fit residuals?\n",
    "- Extracting the fit uncertainty on parameters\n",
    "- Likelihood\n",
    "- Interpreting Likelihood\n",
    "- Getting things to fit: Chi-by-eye and More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f785f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Importing Data (Colab Only)</h3>\n",
    "\n",
    "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n",
    "\n",
    "See the source and attribution information below:\n",
    "\n",
    ">data: data/L05/sn_z_mu_dmu_plow_union2.1.txt <br>\n",
    ">source: http://supernova.lbl.gov/Union/ <br>\n",
    ">attribution: The Supernova Cosmology Project, arXiv:1105.3470v1 <br>\n",
    ">license type: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html \n",
    "\n",
    ">data: data/L05/events_a4_1space.dat, data/L05/events_a8_1space.dat <br>\n",
    ">source: https://www.auger.org/index.php/science/data <br>\n",
    ">source : https://arxiv.org/abs/1709.07321 <br>\n",
    ">attribution: Pierre Auger Collaboration, arXiv:1709.07321v1 <br>\n",
    ">license type: https://creativecommons.org/licenses/by-sa/4.0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d40cfb",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell02\n",
    "\n",
    "#importing data from git repository\n",
    "\n",
    "!git init\n",
    "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n",
    "!git config core.sparseCheckout true\n",
    "!echo 'data/L05' >> .git/info/sparse-checkout\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b9264",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cells below to import the relevant libraries for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfcfb3",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell01\n",
    "\n",
    "!pip install lmfit\n",
    "!pip install astropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd05771",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell02\n",
    "\n",
    "import numpy as np               #https://numpy.org/doc/stable/\n",
    "import matplotlib.pyplot as plt  #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n",
    "import csv                       #https://docs.python.org/3/library/csv.html \n",
    "import math                      #https://docs.python.org/3/library/math.html\n",
    "from scipy import stats          #https://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd69486",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34992da6",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell02\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c49c2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.1 What Do We Call Uncertainty?</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_0) | [Exercises](#exercises_5_1) | [Next Section](#section_5_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6f8c9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10abcf",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_01.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517d1e5",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_01.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc61c1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>What are Fit Residuals?</h3>\n",
    "\n",
    "In the last Lesson, we fit the supernovae data, and got a pretty good fit for Hubble's constant. However, we didn't really try to understand how good our fit was, nor did we try to extract an uncertainty on the fit parameters. To understand what is going on, let's first look at our previous fit to the supernovae data and try to understand residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d1d1d",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "#Let's try to understand how good the fits we made in last Lesson are, let's load the supernova data again\n",
    "label='data/L05/sn_z_mu_dmu_plow_union2.1.txt'\n",
    "\n",
    "\n",
    "def distanceconv(iMu):\n",
    "    power=iMu/5+1\n",
    "    return 10**power\n",
    "\n",
    "def distanceconverr(iMu,iMuErr):\n",
    "    power=iMu/5+1\n",
    "    const=math.log(10)/5.\n",
    "    return const*(10**power)*iMuErr\n",
    "\n",
    "#Now let's zoom in on the small redshift data\n",
    "def load(iLabel,iZMax):\n",
    "    redshift=np.array([])\n",
    "    distance=np.array([])\n",
    "    distance_err=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in plots:\n",
    "            if float(row[1]) > iZMax:\n",
    "                continue\n",
    "            redshift = np.append(redshift,float(row[1]))\n",
    "            distance = np.append(distance,distanceconv(float(row[2])))\n",
    "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n",
    "    return redshift,distance,distance_err\n",
    "\n",
    "redshift=np.array([])\n",
    "distance=np.array([])\n",
    "distance_err=np.array([])\n",
    "redshift,distance,distance_err = load(label,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445621a",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell02\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Now let's run the regression again\n",
    "def variance(isamples):\n",
    "    mean=isamples.mean()\n",
    "    n=len(isamples)\n",
    "    tot=0\n",
    "    for pVal in isamples:\n",
    "        tot+=(pVal-mean)**2\n",
    "    return tot/n\n",
    "\n",
    "def covariance(ixs,iys):\n",
    "    meanx=ixs.mean()\n",
    "    meany=iys.mean()\n",
    "    n=len(ixs)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/n\n",
    "\n",
    "def linear(ix,ia,ib):\n",
    "    return ia*ix+ib\n",
    "\n",
    "def regress(redshift,distance):\n",
    "    #Let's regress\n",
    "    var=variance(redshift)\n",
    "    cov=covariance(redshift,distance)\n",
    "    A=cov/var\n",
    "    b=distance.mean()-A*redshift.mean()\n",
    "    #Done!\n",
    "    return A,b\n",
    "\n",
    "def plotAll(redshift,distance,distance_err,A,b):\n",
    "    #now let's plot it\n",
    "    xmax=np.max(redshift)\n",
    "    xvals = np.linspace(0,xmax,100)\n",
    "    yvals = []\n",
    "    for pX in xvals:\n",
    "        yvals.append(linear(pX,A,b))\n",
    "\n",
    "    #Plot the line\n",
    "    plt.plot(xvals,yvals)\n",
    "    plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "    plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "    plt.ylabel('distances(parsec)', fontsize=15)#Label y\n",
    "    plt.show()\n",
    "    #Print it out\n",
    "    print(\"Hubbles Constant:\",1e6*3e5/A,\"intercept\",b)#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n",
    "\n",
    "A,b=regress(redshift,distance)\n",
    "plotAll(redshift,distance,distance_err,A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d763d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "Now that we have loaded and fit the data again, we would like to actually understand how good the fit is. To do that, we are going to define the residual in $y$. We can define this as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = f(x_{i})-y_{\\rm true} = \\hat{y}_{i}-y_{\\rm true}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Alternatively, we could also divide the above by the uncertainty $\\sigma$. For now, let's compute it for this data, and make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20dd4d1",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell03\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def residual(func,args,redshift,distance,distance_err=[]):\n",
    "    residuals=np.array([])\n",
    "    for i0 in range(len(redshift)):\n",
    "        pResid=func(redshift[i0],args[0],args[1])-distance[i0]\n",
    "        if len(distance_err) > 0:      \n",
    "            pResid=pResid/distance_err[i0]\n",
    "        residuals = np.append(residuals,pResid)\n",
    "    return residuals\n",
    "\n",
    "#This time we are going to look at a histogram of the residuals\n",
    "def plotHist(residuals):\n",
    "    y0, bin_edges = np.histogram(residuals, bins=30)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n",
    "    \n",
    "    #for good measure, let's compare this to a gaussian distribution\n",
    "    k=np.linspace(bin_edges[0],bin_edges[-1],100)\n",
    "    normal=stats.norm.pdf(k,0,residuals.std())\n",
    "    plt.plot(k,normal,'o-')\n",
    "    plt.xlabel(\"y$_{residual}$\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.show()\n",
    "    \n",
    "residuals=residual(linear,[A,b],redshift,distance,distance_err)\n",
    "plotHist(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53235ab0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "So, you can see that our residual distribution looks somewhat like a normal (Gaussian) distribution. As a reminder, here is the analytic form of the normal distribution. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "N(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It has these very important properties, which you can derive yourselves:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[N(x,\\mu,\\sigma)]=\\mu \\\\\n",
    "V[N(x,\\mu,\\sigma)]=\\sigma^2 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, recall from previous Lessons that the *distribution* of the sum of random numbers sampled from any distribution converges to a Gaussian distribution in the large $N$ limit. That means that the noise (i.e., the fluctuations) when taking large enough samples from any random set of distributions should be approximately Gaussian. This is a very powerful statement, that we will use again and again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafc331",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_1'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_1) | [Next Section](#section_5_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbda452",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.1</span>\n",
    "\n",
    "Consider the residual distribution found above, but now divide by the uncertainty of each measurement, that is, following the formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = \\frac{f(x_{i})-y_{true, i}}{\\sigma_{y,i}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If we plot this, and it is truly Gaussian, what are the *expected* values of the mean and stdev of the normalized residual distribution? Enter your answer as a list of two numbers with precision 1e-3: `[mean, stdev]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e504b06",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.2</span>\n",
    "\n",
    "Now use the data to compute the mean and standard deviation of the distribution defined by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = \\frac{f(x_{i})-y_{true, i}}{\\sigma_{y,i}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Hint: There is an option to do this in the function that we previously defined.**\n",
    "\n",
    "How do the actual values compare to the values of an ideal Gaussian? Enter your answers found using the data as a list of two numbers with precision 1e-3: `[mean, stdev]`.\n",
    "\n",
    "\n",
    "You may wish to use the starting code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ab1af",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.1.2\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "residuals=#YOUR CODE HERE\n",
    "plotHist(residuals)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dff32e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.3</span>\n",
    "\n",
    "Load the supernova data for entries up to a redshift of 0.4. You can do that by completing the line of code below. Now, compute the residuals dividing by the error, as above. What is the mean and RMS of the residuals now? Why are the residuals not a Gaussian shape? \n",
    "\n",
    "Enter your answer as a list of two numbers with precision 1e-3: `[mean, stdev]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f8fb4",
   "metadata": {
    "tags": [
     "py",
     "draft",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.1.3\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "redshift02,distance02,distance02_err = load(label,0.4)\n",
    "\n",
    "#Find these parameters by running the regression using regress()\n",
    "A02,b02=#YOUR CODE HERE\n",
    "\n",
    "plotAll(redshift02,distance02,distance02_err,A02,b02)\n",
    "\n",
    "#plot regression to check\n",
    "residuals02=#YOUR CODE HERE\n",
    "plotHist(residuals02)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals02.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals02.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8c67d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.2 Extracting Uncertainty For Linear Fit</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_1) | [Exercises](#exercises_5_2) | [Next Section](#section_5_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5738156",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bf069",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, given the residuals above for our fit, we would like to deduce the variance of the parameters $A$ and $b$. For a linear regression, we can do this analytically (i.e., with math). We need two new definitions for the derivation.\n",
    "\n",
    "First, define the residual sum of squares, often referred to as the RSS. It is defined as \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\rm{RSS} & = & \\sum_{i=1}^{N} \\left(y_{i} - f\\left(x_{i})\\right) \\right)^2\n",
    "         & = & \\sum_{i=1}^{N} \\left(y_{i} - Ax_{i}-b \\right)^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Furthermore, we can define the mean squared error($\\hat{\\sigma}_{\\rm{MSE}}$) as the average of the RSS. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\sigma}_{\\rm{MSE}} & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\left(y_{i} - f\\left(x_{i})\\right) \\right)^2\n",
    "                        & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\left(y_{i} - Ax_{i}-b \\right)^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Here, we don't divide by $N$ because of the fact that $A$ and $b$ are determined from the data, and thus are actually not free parameters, thereby removing 2-degrees of freedom in the data. To understand this, imagine what $\\hat{\\sigma}_{\\rm{MSE}}$  would be if you fit 2 points. It would be exactly 0 since the line $f(x)$ would just connect the two points, so in fact there are no degrees of freedom of variance. A third point would thus fluctuate about the line with an MSE consistent with one point fluctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf572ce5",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "Now, we want to compute the variance of $A$. To understand the variance calculation, we need to consider which variables are random and which are not. In a linear regression, we fit $f(x_{i})$ with the understanding it cannot predict $y_{i}$ perfectly. However, $x_{i}$ are defined in this construction to be fixed, true numbers. As a consequence, we can consider $y_{i}$ the one true random variable, with a variance defined as $\\rm{RSS}$. \n",
    "\n",
    "\n",
    "To get the variance on $A$ we can use the variance in $y_{i}$. The way to think about this is that $y_{i}=\\hat{y}_{i}+u_{i}$ where $u_{i}$ is a random variable defining the variation of the observed $y_{i}$ with respect to the result predicted by the fit. \n",
    "\n",
    "Now, let's go back to the definition of $A$ from earlier:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "   A & = &  \\frac{\\frac{1}{N} \\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right) \\left(y_{i}-\\bar{y}\\right)}{\\frac{1}{N} \\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2} \\\\\n",
    "     & = & \\sum_{i=1}^{N}  \\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}   \\left(y_{i}-\\bar{y}\\right)\\\\\n",
    "     & = & \\sum_{i=1}^{N}  w_{i} \\left(y_{i}-\\bar{y}\\right)\n",
    "\\end{eqnarray}   \n",
    "$$\n",
    "\n",
    "\n",
    "where we have written the first bit as a weight $w_{i}$ for brevity. Now, let's compute the variance of this as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\rm{Var}(A) & = & \\rm{Var} \\left(\\sum_{i=1}^N (w_iy_i-w_i\\bar y)\\right) \\\\\n",
    "  & = & \\rm{Var} \\left(\\sum_{i=1}^N w_iy_i\\right) = \\sum_{i=1}^N\\rm{Var}(w_iy_i)\\\\\n",
    "  & = & \\sum_{i=1}^N w_i^2 \\rm{Var}(y_i)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where we have used properties of variance and the fact that the quantities $w_i$ are fixed — the $x_i$ are fixed numbers and $\\bar x$ is a constant. We also used the fact that $\\rm{Var}(\\bar y) = 0$ since $\\bar y$ is a single number in our context. Thus, we have that the variance of the above will just be proportional to the variance of the one number $y_{i}$ about its prediction $\\hat{y}_{i}$.\n",
    "\n",
    "Noting that on average, we can write $\\mathrm{Var}(y)=\\left(y_{i}-\\hat{y}_{i}\\right)^2$, we then get the variance of $A$ as a weighted sum of the variance in $y$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\rm{Var}(A) & = & \\sum_{i=1}^{N}  w^{2}_{i} \\left(y_{i}-\\hat{y_{i}}\\right)^2 \\\\\n",
    "              & = & \\sum_{i=1}^{N}  w^{2}_{i} \\rm{Var}(y) \\\\\n",
    "              & = & \\sum_{i=1}^{N}  \\left(\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}\\right)^2 \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}{\\left(\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2\\right)^2} \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{1}{\\left(\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2\\right)} \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{1}{N\\rm{Var}(x)} \\rm{Var}(y) \\\\\n",
    "              & \\rightarrow & \\frac{1}{N-2}\\frac{\\rm{Var}(y)}{\\rm{Var}(x)} \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where, in the last step, we have reduced $N\\rightarrow N-2$ to account for the fact that $A$ and $b$ are determined from the fit. We can then write the variance of $b$ noting:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "b           & = & \\bar{y} - A\\bar{x} \\\\ \n",
    "\\rm{Var}(b) & = & \\rm{Var}(\\bar{y}) + \\rm{Var}(A)\\bar{x}^2 \\\\\n",
    "            & = & \\frac{1}{N} \\rm{Var}(y) + \\rm{Var}(A)\\bar{x}^2\\\\\n",
    "            & \\rightarrow & \\frac{1}{N-2} \\rm{Var}(y) + \\rm{Var}(A)\\bar{x}^2  \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "Recall again that $\\bar{x}$ is not a random variable and so has no variance. Let's now calculate the uncertainties on $A$ and $b$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0d4ae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_2'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_2) | [Next Section](#section_5_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f294e9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 5.2.1</span>\n",
    "\n",
    "Consider a fourth-order polynomial:\n",
    "\n",
    "$$f(x) = a_{4}x^4 + a_{3}x^3 + a_{2}x^2 + a_{1}x + a_{0}$$  \n",
    "\n",
    "If we are using this as our fit function for data consisting of `N` points, how many degrees of freedom do we have? Express your answer in terms of `N`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e040f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 5.2.2</span>\n",
    "\n",
    "For the linear function in the example above, the variance in $A$ was expressed as: $\\rm{Var}(A)=\\frac{1}{N-2}\\frac{\\rm{Var}(y)}{\\rm{Var}(x)}$.\n",
    "\n",
    "What is the variance in $b$, expressed in terms of $\\rm{Var}(y)$,  $\\rm{Var}(x)$, $\\bar{x}$, and $N$? Use the appropriate degrees of freedom, and express $\\rm{Var}(y)$ as `Vary`, $\\rm{Var}(x)$ as `Varx`, and $\\bar{x}$ as `xbar`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dadce46",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.3 Computing Uncertainty</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_2) | [Exercises](#exercises_5_3) | [Next Section](#section_5_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966cc63",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e02ce9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Let's compute the uncertainties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcdac9",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.3-runcell01\n",
    "\n",
    "residuals=residual(linear,[A,b],redshift,distance)\n",
    "VarY=np.sum(residuals**2)/(len(redshift)-2)\n",
    "VarA=VarY/variance(redshift)/(len(redshift)-2)\n",
    "Varb=VarA*(redshift.mean())**2+VarY/(len(redshift)-2)\n",
    "print(\"Hubbles Constant:\",1e6*3e5/A,\"+/-\",1e6*3e5*math.sqrt(VarA)/A/A,\"intercept\",b,\"+/-\",math.sqrt(Varb))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666c608",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Using our findings, we can write the weighted regression uncertainties. We will skip the full derivation and write the answer. The best fit parameters for the weighted regression are\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\bar{y}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{y_{i}}{\\sigma_{i}^2}}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} } \\\\\n",
    " \\bar{x}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{x_{i}}{\\sigma_{i}^2}}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} } \\\\\n",
    " A           & = &  \\frac{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}}\\left(x_{i}-\\bar{x}_{w}\\right) \\left(y_{i}-\\bar{y}_{w}\\right)}{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} \\left(x_{i}-\\bar{x}_{w}\\right)^2} \\\\\n",
    "b            & = & \\bar{y}_{w} - A\\bar{x}_{w}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and the variance for these parameters can be written below as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\sigma^{2}     & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} \\left(y_{i} - Ax_{i}-b\\right)^2 \\\\\n",
    "  \\sigma^{2}_{A} & = & \\frac{\\sigma^{2}}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}} \\left(x_{i}-\\bar{x}_{w}\\right)^2} \\\\\n",
    "  \\sigma^{2}_{b} & = & \\left(\\frac{1}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}}}+\\frac{\\bar{x}^2_w}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}}\\left(x-\\bar{x}_{w}\\right)^2}\\right)\\sigma^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "These very much parallel the variances of the parameters except now we have weighted the various events to reflect their respective uncertainties in the measurement. Let's see how our weighted best fit result changes the value of Hubble's constant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3a5ed",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.3-runcell02\n",
    "\n",
    "weights=np.array([])\n",
    "for pVal in distance_err:\n",
    "    weights = np.append(weights,1./pVal/pVal)\n",
    "\n",
    "#Now let's do it with weights\n",
    "def variance_w(isamples,iweights):\n",
    "    mean=np.average(isamples,weights=iweights)\n",
    "    sumw=np.sum(iweights)\n",
    "    tot=0\n",
    "    for i0 in range(len(isamples)):\n",
    "        tot+=iweights[i0]*(isamples[i0]-mean)**2\n",
    "    return tot/sumw\n",
    "\n",
    "def covariance_w(ixs,iys,iweights):\n",
    "    meanx=np.average(ixs,weights=iweights)\n",
    "    meany=np.average(iys,weights=iweights)\n",
    "    sumw=np.sum(iweights)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=iweights[i0]*(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/sumw\n",
    "\n",
    "def regress_w(redshift,weights,distance):\n",
    "    varw=variance_w(redshift,weights)\n",
    "    covw=covariance_w(redshift,distance,weights)\n",
    "    Aw=covw/varw\n",
    "    bw=np.average(distance,weights=weights)-Aw*np.average(redshift,weights=weights)\n",
    "    return Aw,bw\n",
    "\n",
    "Aw,bw=regress_w(redshift,weights,distance)\n",
    "plotAll(redshift,distance,distance_err,Aw,bw)\n",
    "\n",
    "def resid_w(func,args,distance,weights):\n",
    "    residualsw=np.array([])\n",
    "    for i0 in range(len(redshift)):\n",
    "        pResid=linear(redshift[i0],args[0],args[1])-distance[i0]\n",
    "        residualsw = np.append(residualsw,weights[i0]*pResid**2)\n",
    "    return residualsw\n",
    "\n",
    "residualsw = resid_w(linear,[Aw,bw],distance,weights)\n",
    "sumw=np.sum(weights)\n",
    "rsw=np.average(redshift,weights=weights)\n",
    "sigmaw=np.sum(residualsw)/(len(redshift)-2)\n",
    "VarAw=sigmaw*1./variance_w(redshift,weights)*1./sumw\n",
    "Varbw=VarAw*(rsw)**2+sigmaw/sumw\n",
    "    \n",
    "print(\"Weighted Hubbles Constant:\",1e6*3e5/Aw,\"+/-\",1e6*3e5*math.sqrt(VarAw)/Aw/Aw,\"intercept\",bw,\"+/-\",math.sqrt(Varbw))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n",
    "print()\n",
    "\n",
    "#Now the previous\n",
    "print(\"Unweighted Hubbles Constant:\",1e6*3e5/A,\"+/-\",1e6*3e5*math.sqrt(VarA)/A/A,\"intercept\",b,\"+/-\",math.sqrt(Varb))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1cd36",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_3'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_3) | [Next Section](#section_5_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49023cd8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.3.1</span>\n",
    "\n",
    "Why is the weighted Hubble's constant different from the unweighted one? Select the best answer below:\n",
    "\n",
    "- Points with larger uncertainty pull on the fit-line more than points with smaller uncertainty.\n",
    "- Points with smaller uncertainty pull on the fit-line more than points with larger uncertainty.\n",
    "- It is impossible to know why the fit has changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e26447",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.3.2</span>\n",
    "\n",
    "Compute the difference between the mean and the weighted mean to two significant digits; call this $\\Delta_{\\mathrm{Hubble}}$.\n",
    "\n",
    "Also compute the uncertainty of the difference of the two measurements; call this $\\sigma_{\\mathrm{tot}}$. Assume they are separate, uncorrelated measurements, thus add the uncertainties in quadrature, i.e.:\n",
    "\n",
    "$$\\sigma^2_{\\rm{tot}} = \\sigma^2_{1} + \\sigma^{2}_{2}$$\n",
    "\n",
    "Enter your answer as a list of numbers with precision 1e-2: [$\\Delta_{\\mathrm{Hubble}}$, $\\sigma_{\\mathrm{tot}}$]\n",
    "\n",
    "\n",
    "Are these variations consistent with each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c1fd8",
   "metadata": {
    "tags": [
     "py",
     "draft",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.3.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711f587",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.4 Introduction to Likelihood</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_3) | [Exercises](#exercises_5_4) | [Next Section](#section_5_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a51534",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4a2ae",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_04.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0b88c",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.4-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_04.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f6cc0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Likelihood</h3>\n",
    "\n",
    "It turns out that 40% of all people have a gene that makes their pee smell after eating asparagus. Call this fraction $p$. Let's say we have 100 people and we count 56 people who claim that their pee smells. What is the probability that this occurred, for a given true value of $p$?\n",
    "\n",
    "\n",
    "From the binomial distribution we can write this event as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(56\\mathrm{~smell~}|p)=p^{56}(1-p)^{44}\\frac{100!}{44!56!}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, we can flip this relation and compute the probability of $p$ being some value, given our data that 56 of 100 participants reported their pee smelling.  We call this object the \"likelihood\":\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\mathrm{The~Likelihood}}~P(p~|~56\\mathrm{~smell})=p^{56}(1-p)^{44}\\frac{100!}{44!56!}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "It looks the same, but unlike before, where the probability was a function of data (we assumed $p$ was fixed), now this \"probability\" is a function of the *parameter* $p$, and we leave the data fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592197f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Maximum Likelihood Estimator</h3>\n",
    "\n",
    "If we have some data (the observation of 56/100) and a model (that there's a flat population fraction $p$), the maximum likelihood estimator (MLE) gives an estimate of $p$ by finding the value of $p$ that maximizes the probability of corresponding to the observed data, i.e., the likelihood. Let's compute the maximum likelihood estimate $\\hat p$ of this setup (we will just use the term \"data\" in place of the description \"56 smell\").\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\mathrm{MLE~} P(\\rm{data}|p)&=&\\frac{d}{dp} \\left(p^{56}(1-p)^{44}\\frac{100!}{44!56!}\\right) = 0 \\\\\n",
    "                 &=& 56 \\left( p^{55}(1-p)^{44}\\right) -44 \\left(p^{56}(1-p)^{43}\\right) = 0 \\\\\n",
    "                               &=& 56 \\left(1-p\\right) -44 \\left(p\\right) = 0 \\\\\n",
    "              &=& 56 - 100 p = 0 \\\\\n",
    "\\hat p_{\\mathrm{MLE}}             &=& \\frac{56}{100}            \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Given the data we used, this result should not be surprising! So, the maximum likelihood differs from what we thought for our distribution (i.e., 40%). For a simple model and data input like this, one can reason that we are most likely to find that 56/100 people have the asparagus pee trait when the true population fraction is 56/100. But for more complex systems, it's less clear. For instance, in this scenario we are varying $p$ and not varying the actual decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c5cab",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Log Likelihood</h3>\n",
    "\n",
    "For completeness, we can also write the log likelihood. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\rm ~The~Log~Likelihood~} \\log\\left(P(\\rm{data}|p)\\right)= 56 \\log p + 44 \\log(1-p)+\\log\\left(\\frac{100!}{44!56!}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We often write things in terms of $\\log$ (which is base $e$) since probabilities can vary by large absolute values and $\\log$ helps to mitigate the large variations. Also, since $\\log$ is positive-definite and we can construct a one-to-one mapping, minimizing $\\log(f)$ equates to minimizing $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efeefd7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_4'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_4) | [Next Section](#section_5_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eced2c1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.4.1</span>\n",
    "\n",
    "What is the log likelihood evaluated at $p=40$%, given that you observe 56/100 people who claim they can smell asparagus after they pee?\n",
    "\n",
    "You can use the starting code below, if you wish.\n",
    "\n",
    "Report your answer with precision 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7e40a",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.4.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def prob(p,nobs,ntrials):\n",
    "    #nobs: the number of positive observations\n",
    "    #ntrials: the total number of observations (trials)\n",
    "    return #YOUR CODE HERE\n",
    "\n",
    "print(\"Probability is \",prob(0.4,56,100))\n",
    "print(\"Log Likelihood is\", np.log(prob(0.4,56,100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80dbbd",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 5.4.1a (ungraded)\n",
    ">\n",
    ">Plot the likelihood and log likelihood as a function of $p$, given the observation of a 56/100 result. Alternatively, plot the probability of attaining a given outcome vs. the number of positive outcomes (in other words, varying $x$, the number of positive results). How are these plots different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccd774",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.5 An Example: Auger Data (Part 1)</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_4) | [Exercises](#exercises_5_5) | [Next Section](#section_5_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756f22c",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ba1e8",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_05.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589e5c4",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_05.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3691f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Let's consider a Poisson process: high energy cosmic rays arriving on the Earth per unit area. This is known as the cosmic ray flux. Let's analyze data from the Auger experiment: https://www.auger.org/index.php/science/data.\n",
    "\n",
    "Before we go into the physics of cosmic rays, let's look at the data. As mentioned previously, we can envision cosmic ray data as arising from a Poisson process. The arrival of cosmic ray particles of a certain energy should have some measurable rate that corresponds to that of a Poisson process. Think of it as the question: What is the rate at which particles of a certain energy are detected?\n",
    "\n",
    "For the data, we are going to make some cool plots in visual coordinates. The code may look scary, but don't get too stressed; it's just coordinate transformations and nothing deeper.\n",
    "\n",
    "Moving forward, we are plotting things in coordinates of RA (right ascension) and Dec (declination), which measure location on the sky. They are essentially spherical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2ed58",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Let's say we have\n",
    "label='data/L05/events_a8_1space.dat'\n",
    "\n",
    "def rad(iTheta):\n",
    "    return iTheta/180. * math.pi\n",
    "\n",
    "def rad1(iTheta):\n",
    "    return iTheta/180. * math.pi-math.pi\n",
    "\n",
    "def exposure(dec):\n",
    "    theta_max = np.radians(60) # Maximum zenith angle in the dataset\n",
    "    l = np.radians(-35.23) # Latitude of the center of the array (near Malargüe - Argentina)\n",
    "    arg = (np.cos(theta_max) - np.sin(l)*np.sin(dec)) / (np.cos(l)*np.cos(dec))\n",
    "    hm = np.arccos(arg.clip(-1, 1))\n",
    "    return np.cos(l)*np.cos(dec)*np.sin(hm) + hm*np.sin(l)*np.sin(dec)\n",
    "\n",
    "def load(label):\n",
    "    dec=np.array([])\n",
    "    ra=np.array([])\n",
    "    az=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile,delimiter=' ')\n",
    "        for pRow in plots:\n",
    "            if '#' in pRow[0] or pRow[0]=='':\n",
    "                continue\n",
    "            dec = np.append(dec,rad(float(pRow[2])))\n",
    "            ra  = np.append(ra,rad1(float(pRow[3])))\n",
    "            az  = np.append(az,rad(float(pRow[4])))\n",
    "    return dec,ra,az\n",
    "\n",
    "\n",
    "dec,ra,az = load(label)\n",
    "\n",
    "#Let's make a plot this is in local coordinates\n",
    "color_map = plt.cm.Spectral_r\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.add_subplot(111, projection='mollweide')\n",
    "w=exposure(dec) #correct for the exposure rate at the latitude of the observatory.\n",
    "image = plt.hexbin(ra, dec, cmap=color_map,gridsize=60, mincnt=1,C=w,reduce_C_function=np.sum)\n",
    "plt.xlabel('R.A.')\n",
    "plt.ylabel('Decl.')\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Now let's plot this in Galactic Coordinates\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units\n",
    "from astropy.coordinates import Galactic\n",
    "\n",
    "coords = SkyCoord(ra=ra, dec=dec, unit='rad')\n",
    "rap = coords.galactic.l.wrap_at(180 * units.deg).radian\n",
    "decp = coords.galactic.b.radian\n",
    "\n",
    "color_map = plt.cm.Spectral_r\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.add_subplot(111, projection='mollweide')\n",
    "image = plt.hexbin(rap, decp, cmap=color_map,gridsize=45, mincnt=1,reduce_C_function=np.sum)\n",
    "\n",
    "plt.xlabel('R.A.')\n",
    "plt.ylabel('Decl.')\n",
    "plt.grid(True)\n",
    "plt.colorbar(image, spacing='uniform', extend='max')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d640c33",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now, if you look at the above code, you will see one tricky piece, which is a correction for the exposure time. This is a latitude based correction aimed at correcting for the fact that most of the events will be seen at the latitude of the detector (at 34 degrees south), which is what you see when we plot the sky.\n",
    "\n",
    "\n",
    "One interesting thing that might not be clear however, is the fact that there is a  variation of the intensity as a function of  the RA. Let's highlight this variation by plotting a histogram of intensity with 30 bins along the axis of RA (units of radians in this plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d6827",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-runcell02\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def plotHist(iData,iNBins=30):\n",
    "    #Ok enough of having fun, let's look at the asymmetry we observe in right asecion\n",
    "    y0, bin_edges = np.histogram(iData, bins=iNBins)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0=len(iData)*(bin_edges[-1]-bin_edges[0])/iNBins\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid',linestyle='none')\n",
    "    plt.xlabel(\"RA\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    plt.show()\n",
    "    return bin_centers,y0\n",
    "\n",
    "print(len(ra))\n",
    "_,_ = plotHist(ra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0a80d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_5'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_5) | [Next Section](#section_5_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395c1cc",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.5.1</span>\n",
    "\n",
    "What is the range of declination (sky coordinates) available in the data (the data are given in units of radians).? Report your answer as a list of numbers, in units of radians, with precision 1e-3: `[dec_min, dec_max]`.\n",
    "\n",
    "Hint: This can be found with a single line of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd236d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.5.2</span>\n",
    "\n",
    "Now let's look at the variations in the intensity as a function of RA for a subset of the data with a certain range of declination. Specifically, select the data with declination above the equator and plot it. Complete the code below to do this.\n",
    "\n",
    "Do you see a trend similar to the one you see in $RA$? Select the best answer from the following:\n",
    "\n",
    "- Yes, the trend is even more clear\n",
    "- No, there is conclusively no trend\n",
    "- There might be a slight trend, but the uncertainties are too large to tell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b45631",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.5.2\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "ranew=#YOUR Code here\n",
    "\n",
    "_,_ = plotHist(ranew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8418788",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 5.5.2a (ungraded)\n",
    ">\n",
    ">Systematically explore different slices of the data within the range `[dec_min, dec_max]`. Are there any trends that you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbdc39",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.6 An Example: Auger Data (Part 2)</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_5) | [Exercises](#exercises_5_6) | [Next Section](#section_5_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aaf7a7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid6\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10f941",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_06.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effe347",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_06.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4599fdd2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Normally, there are some corrections to this distribution to account for non-uniformities in angle of the detector. Note that these non-uniformities are not so large in RA($\\phi$) for the simple fact that the Earth rotates. So, we will neglect them for this study. However, in reality you can correct for these by building a simulation. \n",
    "\n",
    "Ok, so even without corrections we see that negative RA has more events than positive RA. Let's zoom in on this some more, and count the events. Recall that this is a Poisson process, so the uncertainty on any number $N$ is given by $\\sigma_{N}=\\sqrt{N}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f10c3",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-runcell01\n",
    "\n",
    "#Let's say we have\n",
    "NLeft=0\n",
    "NRight=0\n",
    "for i0 in range(len(ra)):\n",
    "    if ra[i0] < 0:\n",
    "        NLeft+=1\n",
    "    else:\n",
    "        NRight+=1\n",
    "print(\"NLeft:\",NLeft,\"+/-\",math.sqrt(NLeft),\"NRight:\",NRight,\"+/-\",math.sqrt(NRight),\"total/2\",len(ra)/2.)\n",
    "\n",
    "plotHist(ra,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c884d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "On the left hand side, we see nearly 16600 events over a period of 10 years of operation of Auger. If we take this as a rate, we have that $\\lambda=np=16600$, which means that this process behaves as a Poisson process with maximum likelihood at $\\lambda=16600$. To see this, let's write out a Poisson distribution. We can imagine we have $N$ processes that have a probability of $1$ event in the time period the data was taken. The likelihood is just the probability of all $N$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\frac{\\lambda^{N}}{N!} e^{-\\lambda}\\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The log-likelihood of this distribution is just the log of this. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\log(\\mathcal{L})=N\\log(\\lambda)-\\lambda-\\log(N!) \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Optimizing this likelihood distribution, we have\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d\\log(\\mathcal{L})}{d\\lambda}=0\\\\\n",
    "\\frac{N}{\\lambda}-1=0\\\\\n",
    "\\lambda=N\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "So, the maximum likelihood for $\\lambda$ is just the number of events we observe in the data. Hence we can use that for the aggregate or split distribution to deduce the deviation. Let's compute this by computing the p-value ratio of the most likely occurence of a Poisson process, and the one actually observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a86b11",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-runcell02\n",
    "\n",
    "def pvalues(NLeft,NRight):\n",
    "    #Let's compute it\n",
    "    lamb=(NLeft+NRight)/2. #average number of events\n",
    "    pleft=stats.poisson.pmf(NLeft,lamb) #probability of left given averge\n",
    "    pright=stats.poisson.pmf(NRight,lamb) #probability of right given averaged \n",
    "    pcheck=stats.poisson.pmf(int(lamb),lamb)#Most likely probability \n",
    "    print(\"Likelihood Ratio-left\",pleft/pcheck,\"Likelihood Ratio-right\",pright/pcheck,\"check\",pcheck/pcheck)\n",
    "pvalues(NLeft,NRight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd6a96",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "If we predict the rate in right ascension to be uniform, then the number of events that occur at negative (left) RA should be equal to the number of events that occur at positive (right) RA. Let's take the average number of events between left and rights RA to be the expected value of events that occur, if the distribution of events is uniform. Thus, we have that $N_{\\mathrm{avg}}=16093.5$.\n",
    "\n",
    "In this case, the likelihood for $N_{\\mathrm{left}}$ is $p=3\\times10^{-4}$ less likely that than the average value. Likewise with the right.\n",
    "\n",
    "In the following questions we will think a bit more about this deviation, and then perform a similar analysis with a different data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc36e8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_6'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_6) | [Next Section](#section_5_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21567cf0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.6.1</span>\n",
    "\n",
    "Consider the analysis that we just performed, where we split the data into left and right RA. How many standard deviations is the count in the left bin, $N_{\\mathrm{left}}$, from the expected (or average) count, $N_{\\mathrm{avg}}$, assuming a uniform rate? Recall that $N_{\\mathrm{avg}}=16093.5$ and $N_{\\mathrm{left}} = 16600 \\pm 128.84$.\n",
    "\n",
    "Calculate the same number for the right bin, and report your answer as a list of two positive numbers `[num_stdev_left, num_stdev_right]`, with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29153c",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.6.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e158430",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.6.2</span>\n",
    "\n",
    "So far, we've been working with the high energy data. Now load the data at lower energy (4-8 EeV cosmic rays) `events_a4_1space.dat`. As we did previously, plot the RA variation using 30 bins, then make a plot using two bins (splitting the data into left and right sides).\n",
    "\n",
    "Compute the likelihood ratio for the left data bin and the right data bin, as we did previously. Complete the code below to help do this, or write your own.\n",
    "\n",
    "What is the probability that we observe the number of counts in each data bin (left and right), assuming that the rate of counts is uniform in RA? Report your answer as a list of two positive numbers `[likelihood_ratio_left, likelihood_ratio_left]`, with precision 1e-3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73bb65",
   "metadata": {
    "tags": [
     "py",
     "learner_chopped",
     "draft"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>PROBLEM: L5.6.2\n",
    "\n",
    "label_2='data/L05/events_a4_1space.dat'\n",
    "dec_2,ra_2,az_2=load(label_2)\n",
    "\n",
    "#compute asymmetry\n",
    "NLeft_2=0\n",
    "NRight_2=0\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "#PRINT THE COUNTS WITH UNCERTAINTY\n",
    "\n",
    "#PRINT THE PROBABILITY (LIKELIHOOD RATIO)\n",
    "\n",
    "#PLOT IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea8cf6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.7 Log-Likelihood and Chi-square</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_6) | [Exercises](#exercises_5_7) | [Next Section](#section_5_8) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc768d3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid7\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685ed3b",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_07.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17072013",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_07.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7e25a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "<h3>Interpreting Likelihood</h3>\n",
    "\n",
    "In the above, we compared the likelihood of two distributions from a flat hypothesis. We did this by considering the likelihood of a deviation from flat. We see that lower energy particles are pretty close to flat, whereas higher energy particles start to deviate from flat. As a physicist, this is when you start to think WTF. Let's talk about the physics implications later, but for now let's understand the statistics a little more. Let's ask a profound question. How can we improve the sensitivity of this measurement? \n",
    "\n",
    "**Use more than just the left and right sides of the distribution.**\n",
    "\n",
    "To extend the sensitivity, what we can do next is to define a likelihood for an arbitrary distribution. We can define the likelihood over a number of points by imagining that instead of just left and right, each bin is a specific measurement that we perform $N^{events}_{bin}$ times. From this, we can consider the probability of each bin given an expected mean prediction for that bin. For $N$ bins where for bin $i$ we have $x_{i}$ number of events and having the same mean prediction, we can write the likelihood as the multiplication of $N$ Poisson experiments each with the same predicted number of events. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} p(x_{i}|\\lambda) \\\\\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} \\frac{\\lambda^{x_{i}}}{x_{i}!}e^{-\\lambda} \\\\\n",
    "\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i-1}^{N}  x_{i}\\log(\\lambda)-\\log(x_{i}!)-\\lambda\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If we want to optimize this for a specific $\\lambda$, then we just take the derivative\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{d}{d\\lambda}\\log(\\mathcal{L}(x|\\lambda))&=&0=\\sum_{i-1}^{N}  \\frac{x_{i}}{\\lambda}-1 \\\\\n",
    " N\\lambda&=&\\sum_{i-1}^{N} x_{i} \\\\\n",
    " \\lambda=\\bar{x}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Surprise, surprise! The optimal value $\\lambda$ is just the average over all the bins.\n",
    "\n",
    "Moreover, we can compute the variance of $\\lambda$ from its definition. In this case we get \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{\\lambda}^2=\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} x_{i}\\right)\\\\\n",
    "\\sigma_{\\lambda}^2=\\frac{1}{N^2}\\sum^{N}_{i=1}\\mathrm{Var}(x_{i}) \\\\\n",
    "\\sigma_{\\lambda}^2=\\frac{1}{N^2}N\\bar{x}=\\frac{\\bar{x}}{N}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's compute the likelihood of the above distributions using all of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb212b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-runcell00\n",
    "\n",
    "#For the purposes of this section, we will use the low energy data.\n",
    "#Load it here, where we will redefine dec, ra, and az\n",
    "\n",
    "label_2='data/L05/events_a4_1space.dat'\n",
    "dec,ra,az=load(label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fa9a0",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-runcell01\n",
    "\n",
    "_,xis=plotHist(ra)\n",
    "\n",
    "#Now let's define the log of a Poisson distribution (see above)\n",
    "def logpoisson(lamb): #x is our lambda and y0 is our data\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        test = math.factorial(xi)\n",
    "        lTot = xi*np.log(lamb) - lamb  - math.log(test) + lTot\n",
    "    return -1.*lTot\n",
    "\n",
    "#Now lets take the mean of this distribution and compute labmda\n",
    "lamb=xis.mean()\n",
    "print(\"Log Likelihood\",logpoisson(lamb),\"Regular Likelihood\",np.exp(logpoisson(lamb)))\n",
    "x = np.linspace(lamb*0.75, lamb*1.55, 100)\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.plot(x, logpoisson(x));\n",
    "\n",
    "#finally let's compute the minimum of this distribution\n",
    "from scipy import optimize as opt\n",
    "sol=opt.minimize_scalar(logpoisson, method='Brent')\n",
    "print(\"minimum found:\",sol,\"Mean:\",lamb.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0282d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "By minimizing the likelihood we are finding the optimum position of our parameters. Note that the likelihood is some giant number, but the log likelihood is not large (this is because we are multiplying a lot of small probability numbers). This situation isn't so unusual, so perhaps you can see why we prefer working with log likelihood!\n",
    "\n",
    "Now, let's go back to our test. We would like to test this variation in the data and see how consistent each bin is with a Poisson distribution. For this to be the case, our absolute likelihood value has to be a reasonable number. So what is a reasonable number?\n",
    "\n",
    "Here is where we have to rely on some clever trickery. What we are going to do is invoke the central limit theorem, and state that since we are dealing with large numbers, the distribution of our sample about its expectation is going to be Gaussian with standard deviation given for a Poisson distribution. Then, we can compute the log likelihood. Let's do it for each of our bins. \n",
    " \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} p(x_{i}|\\lambda) \\\\\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}} \\\\\n",
    "-\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i-1}^{N}  -\\frac{1}{2}\\log(2\\pi\\sigma^2)+\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The above is for $N$ Gaussian distributed bins. For $N$ Poisson distributed bins we can approximate it as a Gaussian with mean $\\lambda$ and $\\sigma=\\lambda$, and so we can write\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\Pi_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\lambda}}e^{\\frac{-(x_{i}-\\lambda)^2}{2\\lambda}} \\\\\n",
    "-\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i-1}^{N}  -\\frac{1}{2}\\log(2\\pi\\lambda)+\\frac{(x_{i}-\\lambda)^2}{2\\lambda}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Recall from above using the Poisson form we had\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i-1}^{N}  x_{i}\\log(\\lambda)-\\log(x_{i}!)-\\lambda\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If we equate the two we see they are a bit different, and as a consequence our likelihood value will be different. Furthermore there is something else surprising and interesting about this. Let's look at just one term $N=1$ (not just for Poisson).\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(x|\\lambda) &=&-\\frac{1}{2}\\log(2\\pi\\sigma^2)+\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "E[f(x)]      &=&-\\frac{1}{2}\\log(2\\pi\\sigma^2)+E[g^{\\prime}(x)]\\\\\n",
    "\\mathrm{where~}g^{\\prime}(x)         &=&\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "E[g^{\\prime}(x)]      &=&\\frac{\\mathrm{Var}(x)}{2\\sigma^2}\\\\\n",
    "E[g^{\\prime}(x)]      &=&\\frac{\\sigma^2}{2\\sigma^2}=1/2\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "We have focused in on this one parameter $g^{\\prime}(x)$ since the other parameter is just a constant calculated from the parameters of the Gaussian distribution. Since we have assumed $x_{i}$ are each Gaussian distributed with mean $\\mu$, we get the expectation for this distribution is zero and the variance is $\\sigma$, yielding 1/2 when considering $g(x)$ over the data. As a consequence we have that $\\log$ of the likelihood using a gaussian will be $\\frac{1}{2}-\\frac{1}{2}\\log(2\\pi\\sigma^2)$. \n",
    "\n",
    "\n",
    "Since the 1st term doesn't affect the optimization it is often dropped. We can also multiply by 2 to make things look nice, again with no effect. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g(x) &=& \\frac{(x_{i}-\\mu)^2}{\\sigma^2}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "For variables which are truly Gaussian distributed, we expect this to be 1. Furthermore, if we sum over many of these variables we get: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g(x) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-\\mu)^2}{\\sigma^2}\\\\\n",
    "E[g(x)] & = & N\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We then expect that for $N$ Gaussian distributed variables this will give us a value of $N$. This expectation allows us to actually test the data. If we run over our data and compute $E[g(x)]$ and it's too small ($\\ll N$), then our value for $\\sigma$ is too large. If our value for $E[g(x)]$ is too large ($\\gg N$) then we don't have a good fit to the data and our value for $\\sigma$ is too small. \n",
    "\n",
    "Furthermore, the sum of $N$ independent Gaussian random numbers with variance 1 in itself makes its own distribution. This is a $\\chi^{2}$ distribution with $N$ degrees of freedom, and $g(x)$ above is referred to as $\\chi^{2}$. \n",
    "\n",
    "For a Poisson distribution, we can further simplify this to the classic $\\chi^{2}$ that we physicists know and love and copiously use. \n",
    " \n",
    " \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\chi^{2}(x) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-\\mu)^2}{\\mu}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We are going to talk a lot more about $\\chi^{2}$ distributions. However, an important thing to note is that \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E[\\chi^{2}(x)] &\\approx& N \\\\\n",
    "E[\\chi^{2}(x)/N] &\\approx& 1 \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This last relation is known as the normalized $\\chi^{2}$, which we expect to approach $1$ in the large $N$ limit. This is often why fits report the normalized $\\chi^{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9390d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_8'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.8 Minimizing</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_7) | [Exercises](#exercises_5_8) | [Next Section](#section_5_9) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29badfde",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid8\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355a956",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_08.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c6d3b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_08.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9400e",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-runcell01\n",
    "\n",
    "#Log likelihood of a gaussian distribution of our data from above\n",
    "def loggaus(lamb):\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        lTot = lTot+(0.5/(lamb+1e-5))*(xi-lamb)**2\n",
    "        lTot = lTot-0.5*np.log(math.pi*2*lamb)\n",
    "    return lTot\n",
    "\n",
    "#chi2 distribution of our data from above.  \n",
    "def chi2(lamb):\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        lTot = lTot+(1./(lamb+1e-10))*(xi-lamb)**2\n",
    "    return lTot\n",
    "\n",
    "\n",
    "lamb=xis.mean()\n",
    "print(\"Gaussian Likelihood at minimum\",loggaus(lamb),lamb,len(xis))\n",
    "print(\"chi2 value at minium\",chi2(lamb))\n",
    "\n",
    "x = np.linspace(lamb*0.75, lamb*1.55, 100)\n",
    "plt.plot(x, loggaus(x),label='gaus');\n",
    "plt.plot(x, chi2(x)/2.,label='chi2/2');\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#Now let's minimize the two\n",
    "from scipy import optimize as opt\n",
    "sol1=opt.minimize_scalar(loggaus, method='Brent')\n",
    "print(\"Gaussian Minimum\",sol1.x)\n",
    "sol2=opt.minimize_scalar(chi2, method='Brent')\n",
    "print(\"Chi2 Minimum\",sol2.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bdc8a8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "From the above, we see that we can get to the same minimum with either the full Gaussian or the $\\chi^{2}$ distribution. Now, understanding whether your fit is good or not can be completely interpreted by understanding the $\\chi^{2}$ distribution. The $\\chi^{2}$ is a very powerful distribution that we can compute numerically. We can use python tools to understand it, so let's do just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ef737",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-runcell02\n",
    "\n",
    "#Now let's look at our chi2 distribution and see how this compares\n",
    "x = np.linspace(0,80)\n",
    "chi2d=stats.chi2.pdf(x,30) # w0 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2(lamb), c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"$\\chi^{2}(x)$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137824e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "For our fit above, we see that the value we get for our $\\chi^{2}$ calculation is 30, and we see that for a $\\chi^{2}$ distribution that is the sum of 40 independent bins, a value of 30 has a relatively large p-value in the sense that it is close to the peak and we can see it on the blue line. We will talk more about how to interpret these values, but let's move forward with this. However, for now, we can  conclude that a flat line, Poisson-like hypothesis for this data is a good description. \n",
    "\n",
    "Now, before we go to look at a bad fit, I want to go back to the variance of the distribution. Above, we found $\\sigma_{\\lambda}^{2}=\\frac{\\bar{x}}{N}$. What if we can obtain this directly from our minimization?\n",
    "\n",
    "With a little bit of math (that we will skip here) we can compute:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Var}(\\chi^{2}(x))=2N\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which means that for $\\Delta \\chi^{2}(x)=|\\chi^{2}(x)-\\chi^{2}(x\\pm\\sqrt{2N})|$, we are within a standard deviation of the minimum. Imagine living on the line of this distribution and considering moving away from the minimum. Well, if we move away from the minimum, our chances of having a distribution that has a $\\chi^{2}$ value that is not exactly at the minimum is still quite likely provided we are in this $2N$ range. \n",
    "\n",
    "**Presuming that $\\lambda$ is itself Gaussian, we have that $\\lambda$ also follows a $\\chi^{2}$ distribution of the model. We will write this in terms of an expansion of a parameter $\\mu$ about the best fit $\\lambda$, which we denote $\\mu_{0}$.** A $\\chi^{2}$ distribution is a sum of Gaussians, so 1 Gaussian is still a $\\chi^{2}$ distribution. \n",
    "\n",
    "Consequently, $\\Delta \\chi^{2}$ varying our one parameter($\\lambda$) corresponds to varying a $\\chi^{2}$ distribution of 1-degree of freedom. This means that we can write\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\rm{Var}(\\chi^{2}_{1}) = 2 \\\\\n",
    "\\chi^{2}_{1}(X)  = \\left(\\frac{\\mu-\\mu_{0}}{\\sigma}\\right)^{2}\\\\\n",
    "\\rm{a~deviation~of~+1~in~}\\chi~\\rm{corresponds~to}\\\\\n",
    "\\rm{\\Delta \\chi_{1}^2} = 2 \\Delta \\log \\mathcal{L} = 1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for 1 varying parameter (1 degree of freedom). What this means is that the variance on our measured value of $\\lambda$ can be found by varying just the parameter $\\lambda$ looking at the $\\chi^{2}$ value and requiring that $\\Delta \\chi^{2}$ from the minimum be $1$. The positions below and above $\\mu_0$ where $\\chi^2$ is 1 above the minimum correspond exactly to the $+1$ and $-1$ standard deviation of the value $\\mu$. \n",
    "\n",
    "Let's dig a little deeper, to see this through a Taylor expansion of $\\chi^{2}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\mu)=\\chi^{2}_{min}(x_{i},\\mu_{0})+\\frac{1}{2}\\frac{\\partial^{2}}{\\partial \\mu^{2}}\\chi^{2}_{min}(x_{i},\\mu_{0})(\\mu-\\mu_{0})^{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\lambda}$ is the best fit of $\\lambda$ at the minimum. \n",
    "\n",
    "Now, imagine we deviate up from the minimum of the $\\chi^{2}$ by values $\\sigma_{\\mu}$, we know that these deviations need to correspond to $\\frac{1}{2}$.  Since we can write out the full form with Poisson uncertainty, let's solve\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{\\Delta \\chi^2}& = & 1 = \\sum_{i=1}^N\\frac{(\\mu-\\mu_{0} + \\sigma)^{2}}{\\sigma}-\\sum_{i=1}^N\\frac{(\\mu-\\mu_{0})^{2}}{\\sigma}\\\\\n",
    "1 & \\approx & \\sum_{i=1}^N\\frac{(\\mu-\\mu_{0}+\\sigma)^{2}}{\\sigma}-\\sum_{i=1}^N\\frac{(\\mu-\\mu_{0})^{2}}{\\sigma}\\\\\n",
    "1 & = & \\frac{1}{\\sigma}\\sum_{i=1}^N (\\mu-\\mu_{0}+\\sigma)^{2}-(\\mu-\\mu_{0})^{2}\\\\\n",
    "1 & = & \\frac{1}{\\sigma}\\sum_{i=1}^N \\sigma^{2}+2\\sigma(\\mu-\\mu_{0}) \\\\\n",
    "1 & = & \\frac{N\\sigma^{2}}{\\sigma} \\\\\n",
    "\\sigma^{2} & = & \\frac{\\mu_{0}}{N}   \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "just as in derivation of $\\sigma_{\\lambda}^{2}$ above. This gives us a strong confirmation of the Taylor expansion approach that we can verify empirically as well. Through this approach, we also immediately get the correspondence in the variation of the $\\chi^{2}$ that $\\frac{\\partial^{2} \\chi^{2}}{\\partial \\theta^{2}}=\\frac{2}{\\sigma^2_{\\theta}}$. This is known as Wilk's theorem (https://www.jstor.org/stable/2957648?seq=1), and is related closely to a more general form known as Wald's test. It is more generally written in terms of the likelihood as \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma^{2}_{\\theta} = \\left(\\frac{\\partial^{2}\\log \\mathcal{L}}{\\partial\\theta^{2}}\\right)^{-1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For multiple parameters, we write this as a vector $\\vec{\\theta_{i}}$, we can expand this in terms of the Hessian$\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}}$, the Taylor expansion is as follows\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's go back to our $\\chi^{2}$ minimum function and compute the +1$\\sigma$ variation and the -1$\\sigma$ variations of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264048d8",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-runcell03\n",
    "\n",
    "def chi2min(ilamb):\n",
    "    minchi2=chi2(xis.mean())+1 #This is the minmum over the data\n",
    "    return chi2(ilamb)-minchi2\n",
    "\n",
    "lamb=xis.mean()\n",
    "sol1=opt.root_scalar(chi2min,bracket=[lamb, lamb*1.1],method='brentq')\n",
    "sol2=opt.root_scalar(chi2min,bracket=[lamb*0.9, lamb],method='brentq')\n",
    "print(sol1)\n",
    "print(sol2)\n",
    "print(\"sol1\",xis.mean()+math.sqrt(xis.mean()/len(xis)))\n",
    "print(\"sol2\",xis.mean()-math.sqrt(xis.mean()/len(xis)))\n",
    "\n",
    "\n",
    "minlog=chi2(lamb)\n",
    "x = np.linspace(lamb*0.995, lamb*1.005, 50)\n",
    "plt.plot(x, chi2(x),label='chi2');\n",
    "plt.axvline(sol1.root, c='red',label=\"$\\hat{\\lambda}\\pm1\\sigma$\")\n",
    "plt.axvline(lamb, c='blue',label=\"$\\hat{\\lambda}$\")\n",
    "plt.axvline(sol2.root, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc7864",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_8'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_8) | [Next Section](#section_5_9) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665f53d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.8.1</span>\n",
    "\n",
    "Now if we treat $\\chi^2$ as that of a single variable with a variance of $1$ (i.e., for some $\\lambda$), we can write\n",
    "\n",
    "$$\n",
    "\\chi^{2} = \\left(\\frac{\\lambda -\\hat{\\lambda}}{\\sigma_{\\lambda}}\\right)^{2}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\lambda}$ is the best fit and $\\sigma_{\\lambda}$ is the uncertainty in $\\lambda$.\n",
    "\n",
    "What is the $\\Delta \\chi^2$ value from the minimum $\\chi^{2}$ corresponding to two standard deviations in $\\lambda$ from the best fit $\\hat{\\lambda}$? Enter an integer number for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e958e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_9'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.9 Comparison Using lmfit</h2>     \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_8) | [Exercises](#exercises_5_9) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badde946",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS5/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS5_vid9\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ea00",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "<h3>Returning to the High-Energy Data</h3>\n",
    "\n",
    "Now that we have computed a minimum $\\chi^{2}$ and an uncertainty for the rate for the low energy data. What's the minimum, uncertainty, and $\\chi^{2}$ value for the high energy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36633b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_09",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.9-runcell01\n",
    "\n",
    "#solution\n",
    "label8='data/L05/events_a8_1space.dat'\n",
    "dec,ra,az=load(label8)\n",
    "nbins=30\n",
    "_,xis=plotHist(ra,nbins)\n",
    "\n",
    "#compute the mean over the bins\n",
    "lamb=xis.mean()\n",
    "\n",
    "#minimize it\n",
    "sol2=opt.minimize_scalar(chi2, method='Brent')\n",
    "print(\"chi2 minimized value\",lamb.mean())\n",
    "\n",
    "#Now let's look at our chi2 distribution and see how this compares\n",
    "x = np.linspace(0,80)\n",
    "chi2d=stats.chi2.pdf(x,nbins) # 30 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2(lamb), c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#now let's plot the mimum and uncertainty\n",
    "sol0=opt.minimize_scalar(chi2, method='Brent')\n",
    "lamb=sol0.x\n",
    "sol1=opt.root_scalar(chi2min,bracket=[lamb, lamb*1.01],method='brentq')\n",
    "sol2=opt.root_scalar(chi2min,bracket=[lamb*0.99, lamb],method='brentq')\n",
    "print(sol0)\n",
    "print(sol1)\n",
    "print(sol2)\n",
    "print(\"sol1\",xis.mean()+math.sqrt(xis.mean()/len(xis)))\n",
    "print(\"sol2\",xis.mean()-math.sqrt(xis.mean()/len(xis)))\n",
    "\n",
    "minlog=chi2(lamb)\n",
    "x = np.linspace(lamb*0.99, lamb*1.01, 50)\n",
    "plt.plot(x, chi2(x),label='chi2');\n",
    "plt.axvline(sol1.root, c='red',label=\"$\\hat{\\lambda}\\pm1\\sigma$\")\n",
    "plt.axvline(lamb, c='blue',label=\"$\\hat{\\lambda}$\")\n",
    "plt.axvline(sol2.root, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\log(\\mathcal{L}(\\lambda))$\")\n",
    "#plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435ee23",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "<h3>Getting things to fit: Chi-by-eye and More</h3>\n",
    "\n",
    "In reality, all of this stuff can be seen just by looking at the plots by eye. Let's actually visualize all of what we just did with the high and low energy data. Just to be a bit lazy, we are going to do this with `lmfit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5798b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_09",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.9-runcell02\n",
    "\n",
    "import lmfit\n",
    "#Plot a constant function\n",
    "def f(x,a):\n",
    "    return a\n",
    "\n",
    "def prephist(iRA):\n",
    "    y0, bin_edges = np.histogram(iRA, bins=30)\n",
    "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    y0 = y0.astype('float')\n",
    "    return x0,y0,1./(y0**0.5)\n",
    "\n",
    "def plothist(iRA):\n",
    "    x,y,xweights=prephist(iRA)\n",
    "    model  = lmfit.Model(f)\n",
    "    p = model.make_params(a=1000)\n",
    "    result = model.fit(data=y,x=x, params=p, weights=xweights)\n",
    "    lmfit.report_fit(result)\n",
    "    print(result.params.items(),result.params[\"a\"].value)\n",
    "    plt.errorbar(x,y,yerr=y**0.5,c='black',marker='.',linestyle = 'None')\n",
    "    x = np.linspace(x[0],x[-1], 50)\n",
    "    y=np.array([])\n",
    "    for pX in x:\n",
    "        pOut=f(pX,result.params[\"a\"].value)\n",
    "        y=np.append(y,pOut)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"RA\")\n",
    "    plt.ylabel(\"$N_{events}$\")\n",
    "    plt.show()\n",
    "    \n",
    "label8='data/L05/events_a8_1space.dat'\n",
    "dec,ra8,az=load(label8)\n",
    "plothist(ra8)\n",
    "\n",
    "label4='data/L05/events_a4_1space.dat'\n",
    "dec,ra4,az=load(label4)\n",
    "plothist(ra4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0bcfb",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "So, you can see that all of our fitting actually did all of the computations we did before, except super quickly. Furthermore, we get the $\\chi^{2}$ and the uncertainty. Lastly, and most importantly, we can do the chi-by-eye. You can see in the top plot that most of the points don't cross the line, whereas in the bottom plot, a lot of the points DO cross the line. This is reflective of the fact that our $\\chi^2$ is good for one and bad for other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea87ed6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "### Challenge Question: \n",
    "\n",
    "How often should the points have uncertainties such that they not cross the line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4656786",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_09",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.9-runcell03\n",
    "\n",
    "#Answer: \n",
    "print(\"Answer:\",stats.norm.cdf(0,1)*100,\"% Above the line\")\n",
    "print(\"Answer:\",stats.norm.cdf(0,1)*100,\"% Below the line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3bae9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "We can define the \"Chi-by-eye\" test which states that **if the points mostly line up with the best fit then we are golden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb3eea",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "Now, all the above math of the previous section actually holds in the scenario where we don't just fit a line. We could imagine that every point is Gaussian fluctuated about some predicted form. This means that we can make the assumption that the points will be Gaussian fluctuated around the best possible fit function, no matter what that function is. Let's now try to see if we can fit the above function with a new form given by \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = a + b \\sin(x)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4db3c0",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_09",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.9-runcell04\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b*np.sin(x)\n",
    "    return a+pVal\n",
    "\n",
    "label8='data/L05/events_a8_1space.dat'\n",
    "dec,ra8,az=load(label8)\n",
    "x,y,xweights=prephist(ra8)\n",
    "print(x,y)\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=10)\n",
    "result = model.fit(data=y,x=x, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd81a72",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "The chi2-by-eye looks pretty good here. Also, the actual value of thie $\\chi^{2}$ (26.8) also looks good. Let's plot this to see exactly how good it looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3eec0f",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_09",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.9-runcell05\n",
    "\n",
    "#Now let's look at our chi2 distribution and see how this compares\n",
    "x = np.linspace(0,80)\n",
    "chi2d=stats.chi2.pdf(x,40) # 40 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(result.chisqr, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6d9a3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_09"
    ]
   },
   "source": [
    "Thats great! It's in the bump and not on the tails. If anything our uncertainties are too large. \n",
    "\n",
    "So, did we get the right function? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483beec",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_9'></a>   \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_9) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abd50e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.9.1</span>\n",
    "\n",
    "Repeat the preceding fit using the function: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = a x + b \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "What is the $\\chi^{2}$ value? Enter a number with precision 1e-3.\n",
    "\n",
    "Given these results, which fit is better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a75c2",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L5.9.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
